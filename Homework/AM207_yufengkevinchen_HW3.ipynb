{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AM 207**: Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verena Kaynig-Fittkau and Pavlos Protopapas  <br>\n",
    "**Due: 11.59 P.M. Thursday March 17th, 2016**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "+ Upload your answers in an ipython notebook to Canvas.\n",
    "\n",
    "+ We will provide you imports for your ipython notebook. Please do not import additional libraries.\n",
    "\n",
    "+ Your individual submissions should use the following filenames: AM207_YOURNAME_HW3.ipynb\n",
    "\n",
    "+ Your code should be in code cells as part of your ipython notebook. Do not use a different language (or format). \n",
    "\n",
    "+ **Do not just send your code. The homework solutions should be in a report style. Be sure to add comments to your code as well as markdown cells where you describe your approach and discuss your results. **\n",
    "\n",
    "+ Please submit your notebook in an executed status, so that we can see all the results you computed. However, we will still run your code and all cells should reproduce the output when executed. \n",
    "\n",
    "+ If you have multiple files (e.g. you've added code files or images) create a tarball for all files in a single file and name it: AM207_YOURNAME_HW3.tar.gz or AM207_YOURNAME_HW3.zip\n",
    "\n",
    "\n",
    "### Have Fun!\n",
    "_ _ _ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import scipy.stats \n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Employee Satisfaction Improvement\n",
    "This problem is going to explore the differences between complete pooling, unpooling and partial pooling for a normal model with observed standard deviations. \n",
    "\n",
    "You are working for a consulting firm which is trying to find a good strategy to improve employee satisfaction for their customers. Your company ran pilot studies in eight different customer companies and measured the improvement in employee satisfaction after the plan had been implemented for two years. The data you are given is the mean and standard deviation of the satisfaction improvement, measured by a survey. \n",
    "You follow the nature of your data by modeling the effect of the strategie with a normal model. To simplify things you can assume that the different standard deviations are an effect of different sample sizes, and that there is actually one underlying observation variance:\n",
    "\n",
    "$$ \\sigma_j^2 = \\frac{\\sigma^2}{n_j}$$\n",
    "\n",
    "Thus the difference in the observed standard deviations in the survey results are caused by having different numbers of survey answers $n_j$. \n",
    "\n",
    "Discuss, create and compare three different solutions for your model using PYMC or another sampling method of your choice:\n",
    "\n",
    "* complete pooling\n",
    "* unpooling\n",
    "* partial pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here is the data\n",
    "data = np.double(np.array([[29.5,18.4],[6.3,12.7],[-3.9,15.9],[7.2,10.2],[-2.1,9.0],[1.8,12.1],[19.6,7.3],[12,18.6]]))\n",
    "data_means = data[:,0]\n",
    "data_std = data[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Motif Finding Using Gibbs\n",
    "\n",
    "One interesting problem in bioinformatics is that of finding common subsequences of nucleotid bases (these subsequences are called motifs) that repeat themselves within larger DNA sequences. The problem is relevant for genetics because locating the positions of these motifs within the DNA sequence helps in the understanding of how genes are regulated.\n",
    "\n",
    "Suppose that you are a biologist who is analyzing genetic material collected in a nearby asteroid. The DNA of this extraterrestrial form of life is formed as a sequence of $n_B = 5$ nucleotide bases. Let us label these extraterrestrial nucleotides as 'M', '0', '2', 'A', and '7'. You have a DNA sample $\\mathcal{S}$ consisting of $p=20$ sequences of DNA, each of them with a lenght $l=200$ nucleotide bases that you can find in <a href=\"HW3/Sequences_new.dat\">this file</a>. Each row of the file is a DNA sequence. You are asked to find a motif of $q = 5$ consecutive nucleotides hidden in the background that appears to repeat itself very often in the DNA you were given, but that appears only once in each of the sequences.\n",
    "\n",
    "Let us formalize the problem. The starting positions of the motif within each sequence (our missing data in the problem) can be represented by the set of random variables:\n",
    "\n",
    "$$\n",
    "\\mathcal{A} = \\left\\{a_k, k = 1,...,p\\right\\}\n",
    "$$\n",
    "\n",
    "The motif has to start somewhere within the sequences and so for each sequence $\\mathcal{S_k}$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{l}P(a_k=1) = 1\n",
    "$$\n",
    "\n",
    "What we are after is the joint distribution $P(\\mathcal{A}|\\mathcal{S})$ for the motif alignment (i.e., its starting position) being $a_k$ for sequence $\\mathcal{S}_k$. As shown in [this paper](http://www.cs.cmu.edu/~epxing/Class/10810/readings/liu.pdf), for each sequence $\\mathcal{S}_k$ (think of each sequence as a dimension of our parameter space) we can obtain the conditional probability: \n",
    "\n",
    "$$\n",
    "P(a_k=i|\\mathcal{A_{\\hat{k}}},\\mathcal{S})=\\frac{1}{Z}\\prod_{j=1}^q\\left(\\frac{\\hat{\\theta}_j}{\\hat{\\theta}_0}\\right)^{s_{i+j-1}}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{A_{\\hat{k}}}$ refers to the alignments in all sequences other than $\\mathcal{S}_k$, and $Z$ is a normalization factor. Of the other quantities, $s_x$ is a vector index for the *x-th* position in the sequence, with length $q$ and value 1 at the entry corresponding to base $a_k$ and 0 for all other entries. Finally, vectors $\\hat{\\theta_j}$ and $\\hat{\\theta_0}$ contain respectively the probabilities of observing the $q$ bases at the corresponding position of the current sequence, and the probabilities of finding the same bases in the background.\n",
    "\n",
    "Your only task is to design a Gibbs algorithm that samples the joint probability $P(\\mathcal{A}|\\mathcal{S})$. Your algorithm should converge and provide the alignments of the motif and the secret motif itself. Here are some hints/tasks that should help you and that will help the TF grading your homework:\n",
    "\n",
    "(a) Describe the equation for the conditional probabilities in your own words, and make sure you understand it before you code anything. Be as explicit as possible.\n",
    "\n",
    "(b) Start by assigning random starting positions for the $q$-long motif in all sequences. Then exclude a particular sequence $S_k$ (your current sequence) and use the remaining sequences to construct $\\hat{\\theta_j}$ as a probability matrix from counting the number of times that the *i-th* base appears in position $j$ of the motif given the current alignments. Construct a similar matrix $\\hat{\\theta_0}$ from counting the number of times that the *i-th* base appears in the background.\n",
    "\n",
    "(c) From your $\\hat{\\theta_j}$ and $\\hat{\\theta_0}$ matrices, derive the conditional probability over all possible alignments for your current sequence and draw a sample from it. This sample will be your updated alignment for the current sequence. \n",
    "\n",
    "(d) Iterate over all $p$ sequences. Such iteration over sequences is only one Gibbs iteration. After enough Gibbs iterations you should start noticing that the algorithm has converged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Optimizing Hand Luggage\n",
    "\n",
    "You are going on a trip and have to optimize your hand luggage, but thanks to your cheap flight ticket, the weight of the hand lugagge is restricted and you are sure the airline will enforce the upper limit. You have a set of presents that you want to bring to the relatives you are visiting, but you have to notice that they don't all fit into your suitcase. \n",
    "\n",
    "This problem is also called the knapsack problem: given a set of items, each has its weight and value, determine which items should be included into your suitcase, so that the total weight does not exceed some value $W$ and the total value is maximal. The kind of the knapsack problem when each item can be included into the collection at most once is called the 0/1 knapsack problem. Your task is to solve this problem using simulated annealing. \n",
    "\n",
    "Implement simulated annealing to solve this problem with the list of items below. Which ones would you pick for your suitcase? Plot and discuss your optimization scheme and results. \n",
    "Compare your solution to a greedy algorithm, which sorts the items by the ratio $\\frac{v_i}{w_i}$ and puts them into the suitcase consecutively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knapsack problem has the formulation: \n",
    "\n",
    "$\\max \\sum_{i=1}^n v_i x_i$\n",
    "\n",
    "subject to \n",
    "$\\sum_{i=1} ^n w_i x_i \\leq W$\n",
    "\n",
    "Here we can let $x_i \\in {0,1}$ , where 1 represents item i is in the bag and 0 represents item i is out of the bag.\n",
    "\n",
    "We aim to solve for a vector of length N consisting of either 0 or 1s. This problem is NP hard, so we solve the problem using a greedy algorithm and simulated annealing. \n",
    "\n",
    "First, we solve the problem using a greedy algorithm. We rank the items using the ratio $\\frac{v_i}{w_i}$ from largest to smallest. Then we incrementally add the ranked item into our luggage. If current_weight + weight(i) <= W, then we add the item. Otherwise, we skip the item and move onto the next item. In this fashion, we loop through all items once. \n",
    "\n",
    "The greedy method returns the list: [41 77 70 88 80  0 68 58 78  3 38 10 39 92 11 49 96 18 79 50 19 85 32], with value =  897 and weight =  2806. The computational cost is O(N). Please refer to our implementation in the attached code below. \n",
    "\n",
    "For simulated annealing, we implement an algorithm similar to that given in the lecture 12 note. In each step. we randomly choose L items, then for each item we throw a coin to determine whether we change from the current state (i.e take it out of bag or leave it in if it's currently in, vice versa). After this swapping, we check if the proposed solution is feasible, more precisely if we still respect the weight constraint. If not, then we propose again until a valid proposal is given. \n",
    "\n",
    "Next, we follow the standard procedure of calculating the new value. Delta of energy is $\\triangle E = E_{new} - E_{prev}$. The acceptane probability is computed as \n",
    "\n",
    "$P = exp(\\triangle E/T) \\ if \\triangle E < 0$, otherwise, $P =1$. \n",
    "\n",
    "We reheat if the temperature falls below a threshold value. In each epoch we reduce the temperature linearly: $T_{k+1}=\\alpha T_{k}$. There are 3 potential stopping criteria: \n",
    "\n",
    "1. the standard deviation with respect to the mean in the final m points are small\n",
    "2. the total number of iterations exceeds our chosen limit\n",
    "3. the objective function reaches a desired value\n",
    "\n",
    "Here we set 3 to be the sum of all item values. Consequently, the SA method terminates only due to the first 2 conditions. We manually set the SA parameters through experimentation. The SA method returns the list: [ 0  3 10 17 18 19 25 38 39 41 49 50 58 68 70 77 78 79 80 85 88 92 96] with value =  916 weight=  2995. This solution is better than the greedy method. Please refer to the attached code for implementation and convergence plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here is your data:\n",
    "\n",
    "#number_of_data_points\n",
    "N = 100\n",
    "\n",
    "#total_weight_limit \n",
    "W = 3000\n",
    "\n",
    "# weight of all the different items\n",
    "w = np.array([  38,  236,  909,   73,  768,  906,  716,  646,  848,  961,  145,\n",
    "        130,  973,  584,  750,  509,  391,  282,  179,  277,  255,  358,\n",
    "        915,  469,  908,  253,  491,  669,  926,  399,  563,  581,  216,\n",
    "        984,  754,  504,  479,  865,   87,  142,  394,    8,  320,  830,\n",
    "        535,  314,  514,  897,  317,  210,  265,  729,  654,  628,  432,\n",
    "        634,  457,  543,   72,  388,  455,  918,  562,  314,  516,  965,\n",
    "        793,  498,   44,  589,   27,  821,  337,  622,  884,  298,  467,\n",
    "         16,   65,  197,   26,  368,  739,  472,  904,  283,  666,  617,\n",
    "         23,  778,  708, 1000,  127,  280,  382,  357,  156,  934,  314,\n",
    "        596])\n",
    "\n",
    "# value of all different items\n",
    "v = np.array([36, 38, 30, 32, 40, 45, 45, 37, 49, 40, 44, 30, 31, 47, 43, 33, 30,\n",
    "       43, 36, 50, 36, 32, 42, 41, 37, 43, 38, 41, 42, 41, 50, 34, 37, 37,\n",
    "       43, 34, 46, 48, 30, 43, 40, 47, 37, 40, 50, 30, 42, 31, 39, 48, 49,\n",
    "       31, 32, 42, 37, 32, 40, 30, 39, 48, 36, 32, 37, 37, 46, 45, 35, 47,\n",
    "       40, 50, 46, 35, 43, 47, 48, 31, 50, 40, 30, 37, 30, 49, 47, 44, 43,\n",
    "       50, 50, 41, 36, 43, 45, 39, 32, 37, 35, 34, 35, 38, 43, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy solution, total value = 897 total weight =  2806\n"
     ]
    }
   ],
   "source": [
    "#solution of the greedy method\n",
    "ratio_ind = np.argsort(v/(w-0.0))\n",
    "ratio_ind = ratio_ind[::-1]\n",
    "#print ratio_ind\n",
    "\n",
    "#greedy algorithm\n",
    "tot_weight = 0\n",
    "items = np.zeros(N,dtype='int')-1\n",
    "\n",
    "for i in xrange(N):\n",
    "    if tot_weight + w[ratio_ind[i]] <= W:\n",
    "        items[i] = ratio_ind[i]\n",
    "        tot_weight += w[ratio_ind[i]]\n",
    "\n",
    "items = items[items!=-1]\n",
    "\n",
    "print 'greedy solution, total value =', np.sum(v[items]), 'total weight = ', tot_weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy solution is:  [41 77 70 88 80  0 68 58 78  3 38 10 39 92 11 49 96 18 79 50 19 85 32] with value =  897 and weight =  2806\n",
      "current iteration is  1000 out of  20000\n",
      "current weight is:  2877\n",
      "current iteration is  2000 out of  20000\n",
      "current weight is:  2268\n",
      "current iteration is  3000 out of  20000\n",
      "current weight is:  2637\n",
      "current iteration is  4000 out of  20000\n",
      "current weight is:  2793\n",
      "current iteration is  5000 out of  20000\n",
      "current weight is:  2761\n",
      "current iteration is  6000 out of  20000\n",
      "current weight is:  2721\n",
      "current iteration is  7000 out of  20000\n",
      "current weight is:  2942\n",
      "current iteration is  8000 out of  20000\n",
      "current weight is:  2904\n",
      "current iteration is  9000 out of  20000\n",
      "current weight is:  2741\n",
      "current iteration is  10000 out of  20000\n",
      "current weight is:  2968\n",
      "current iteration is  11000 out of  20000\n",
      "current weight is:  2995\n",
      "current iteration is  12000 out of  20000\n",
      "current weight is:  2839\n",
      "current iteration is  13000 out of  20000\n",
      "current weight is:  2995\n",
      "current iteration is  14000 out of  20000\n",
      "current weight is:  2995\n",
      "current iteration is  15000 out of  20000\n",
      "current weight is:  2995\n",
      "current iteration is  16000 out of  20000\n",
      "current weight is:  2995\n",
      "current iteration is  17000 out of  20000\n",
      "current weight is:  2995\n",
      "current iteration is  18000 out of  20000\n",
      "current weight is:  2995\n",
      "ftol\n",
      "simulated annealing solution is:  [ 0  3 10 17 18 19 25 38 39 41 49 50 58 68 70 77 78 79 80 85 88 92 96] with value =  916 weight=  2995\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAFXCAYAAABOYlxEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9fa42a8510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function to swap items in and out of the bag\n",
    "#sol: vector of 1 or 0\n",
    "#L: number of items to be swapped\n",
    "def swap_bags(sol,L):\n",
    "    swapped_sol = sol.copy()\n",
    "    \n",
    "    no_change_prob = 0.5\n",
    "    #swap 1 and 0\n",
    "    for i in xrange(L):\n",
    "        swp_ind = np.random.randint(0,len(sol),1)\n",
    "        if np.random.uniform(0,1,1) > no_change_prob:\n",
    "            swapped_sol[swp_ind] = (swapped_sol[swp_ind] +1 ) % 2\n",
    "            \n",
    "    #print swapped_sol\n",
    "    return swapped_sol\n",
    "\n",
    "#simulated annealing\n",
    "#value: array of item values\n",
    "#cost: corresponding array of item cost\n",
    "#init_sol: initial solution\n",
    "#init_temp: initial temperature\n",
    "#ftol, itol: otol: function, iteration and objective function stopping criteria\n",
    "#reannealing: schedule for reheating (number of iteration)\n",
    "\n",
    "def simulated_annealing(value,cost,init_sol,init_temp,thermostat,ftol,itol,otol,reannealing):\n",
    "    #sol = np.zeros(value.shape)\n",
    "    sol = init_sol\n",
    "    \n",
    "    #length for comparion: \n",
    "    m = 10000;\n",
    "    value_vec =[]\n",
    "    weight_vec = []\n",
    "    prev_value = np.sum(v[init_sol==1])\n",
    "    temperature = init_temp\n",
    "    \n",
    "    #number of accepted steps\n",
    "    it = 0\n",
    "    \n",
    "    #number of iterations\n",
    "    atp = 0\n",
    "    \n",
    "    while it>=0:\n",
    "        #number of items to take in and out\n",
    "        #L = np.max((np.floor(np.sqrt(temperature)).astype(int),1))\n",
    "        L = np.ceil(temperature).astype(int)\n",
    "        #print L\n",
    "        \n",
    "        #propose a FEASIBLE sol\n",
    "        proposed_sol = swap_bags(sol,L)\n",
    "        \n",
    "        #print proposed_sol\n",
    "        while np.sum(w[proposed_sol==1])>W:\n",
    "            #print('re-proposing because exceeds limit')\n",
    "            proposed_sol = swap_bags(sol,L)\n",
    "        \n",
    "        \n",
    "        new_value = np.sum(v[proposed_sol==1])\n",
    "        new_weight = np.sum(w[proposed_sol==1])\n",
    "        delta_value = new_value - prev_value\n",
    "        \n",
    "        #print proposed_sol\n",
    "        #print delta_value\n",
    "        #assert(0)\n",
    "        \n",
    "        #move to new step \n",
    "        if new_value > prev_value or np.random.rand() < np.exp(delta_value/temperature):\n",
    "            sol = proposed_sol.copy()\n",
    "            prev_value = new_value\n",
    "            value_vec.append(new_value)\n",
    "            weight_vec.append(new_weight)\n",
    "            it += 1\n",
    "        \n",
    "        atp += 1\n",
    "        \n",
    "        #check if it is time to cool down\n",
    "        if it % reannealing ==0:\n",
    "            temperature = thermostat * temperature # linear\n",
    "            \n",
    "            #reheat if too cold\n",
    "            if temperature < 0.01:\n",
    "                temperature = 2\n",
    "        \n",
    "        #debugging\n",
    "        if True and not (atp % 1000 ):\n",
    "            print 'current iteration is ', atp, 'out of ', itol\n",
    "            print 'current weight is: ', np.sum(w[proposed_sol==1])\n",
    "        \n",
    "        #terminating condition\n",
    "        if len(value_vec)>m and np.std(value_vec[-m:])/np.mean(value_vec[-m:]) < ftol:\n",
    "            print 'ftol'\n",
    "            break\n",
    "        if atp >itol:\n",
    "            print 'itol'\n",
    "            break\n",
    "        if len(value_vec)> 0 and value_vec[-1] >= otol:\n",
    "            print 'otol'\n",
    "            print value_vec[-1]\n",
    "            break\n",
    "            \n",
    "    #print atp\n",
    "    \n",
    "    return sol,value_vec,weight_vec,float(it)/atp\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #initialize solution (from )\n",
    "    print 'greedy solution is: ', items, 'with value = ', np.sum(v[items]), 'and weight = ', np.sum(w[items])\n",
    "    init_sol = np.zeros(v.shape)\n",
    "    init_sol[items] = 1\n",
    "    \n",
    "    #solve using simulated annealing\n",
    "    init_temp = 10.0\n",
    "    thermostat = 0.90\n",
    "    ftol = 0.01\n",
    "    itol = 20000\n",
    "    otol = np.sum(w)\n",
    "    reannealing = 500\n",
    "    \n",
    "    #start with all zeros\n",
    "    init_sol[:]=0\n",
    "    sa_sol,value_hist,weight_hist, accept_rate = simulated_annealing(v,w,init_sol,init_temp,thermostat,ftol,itol,otol,reannealing)\n",
    "    \n",
    "    item_indices = np.arange(len(v))\n",
    "    print 'simulated annealing solution is: ', item_indices[sa_sol==1], 'with value = ', value_hist[-1], 'weight= ', weight_hist[-1]\n",
    "    \n",
    "    #plot solution\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(value_hist,'r-')\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(weight_hist,'g-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 4: Confusing Classifications\n",
    "\n",
    "You are a graduate student conducting research in image processing.  You want to test out your latest algorithm, which you decide to call \"Ultra-Multilayer Hierarchical Super Convolutionary Neural Network.\"  However, in order to test out your algorithm, you need a sizable training data set. Luckly, your advisor has generously given you over 10 GB of over one million stock photos of cats and dogs.  Unfortunately, none of the images are labelled.  Fantastic!  You enjoy looking at photos of puppies and kittens in your spare time anyways so you decide to dedicate this entire weekend to labeling all of them. \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    " <td><img src=\"HW3/cat.jpeg\" width=150>\n",
    " </td>\n",
    " <td><img src=\"HW3/dog.jpeg\" width=150>\n",
    " </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "However, after spending two hours looking over hundreds of images, these puppies and kittens are no longer looking so cute.  In fact, you are starting to get disgusted at the idea of looking at another picture of these furry creatures.  \n",
    "\n",
    "At this time, you decide to tap into the \"power of the crowd\" by farming out the labeling task to the workers of Amazon Mechanical Turk (MTurk). You decide to hire 3 MTurk workers.  However, you're not sure if these workers are reliable. So, how can you quantify the competency of the workers? In this problem you will develop a model to access the general difficulty of labeling the images of your two classes. \n",
    "\n",
    "Classifying items in general can be hard even for humans.  Some items just look alike, even if they actually are from two different classes. The difficulty in categorizing items of a specific class in relation to other classes is summarized by a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix):\n",
    "\n",
    "$$ \\Theta = \\left( \n",
    "\\begin{array}{cc}\n",
    "1.0 & 0.0 \\\\\n",
    "0.5 & 0.5\n",
    "\\end{array}\n",
    "\\right ) $$\n",
    "\n",
    "This confusion matrix indicates that items of class 1 are very easy to classify, and always correctly labeled, whereas items of class 2 are very hard to classify and labels are basically just random guesses between the two possibilities. \n",
    "\n",
    "The models we discussed so far in homework and lecture were pretty low in dimensions. This problem is going to show you that the number of dimensions can grow very fast for some models, making a good sampling strategy crucial. We will only be able to explore a very minimal version of the problem, because of our limited computational resources, but it should be immediately clear how this model would scale for a greater number of classes, workers, and/or items.\n",
    "\n",
    "Develop a Bayesian model that takes as input a set of (possible erroneous) item labelings and infer the underlying confusion matrix and the true label each data point. To make the model manageable by your laptop, use only one underlying confusion matrix (in principle different people could have different difficulties) two classes for the labels (the confusion matrix is 2x2), and a maximum of 150 data points. \n",
    "\n",
    "The model you are after is displayed in the following dependency graph:\n",
    "<img src=\"HW3/confusion_model.png\" alt=\"confusion matrix model\" width=300>\n",
    "\n",
    "\n",
    "$$\\rho \\sim Dir(\\alpha) $$\n",
    "\n",
    "$$z_i \\sim Multinomial(\\rho)$$\n",
    "\n",
    "\\begin{equation}\n",
    "\\Lambda = \\begin{bmatrix}\n",
    " \\lambda_1 \\\\\n",
    " \\lambda_2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "$$\\Theta_{(k,:)} \\sim Dir(\\lambda_k)$$\n",
    "\n",
    "$$r_{i,j} \\sim Multinomial(\\Theta_{(z_i , : )}), \\ \\forall j \\in \\{1,...,J\\}$$\n",
    "\n",
    "You're given reports generated by the 3 workers, so $r_{i,1}$ is one label for item $r_i$ and $r_{i,2}$ is another label given from another worker. Note that if the confusion matrix is not the identity matrix then these labels can be different because the workers make mistakes. The arrows in the diagram indicate dependence. So the labelings you observe are dependent on the confusion matrix $\\Theta$ and the underlying true label $z_i$ for each item. $N$ is the number of data points you have and as described above you should use $N \\leq 150$.\n",
    "\n",
    "You can see from the diagram that the model uses Multinomials with Dirichlet priors. \n",
    "\n",
    "1. Start by describing the model in terms of these distributions, what they mean and what this arrangement means for the form of the labels $z_i$. Discuss the meaning and influence of the hyperparameter $\\alpha$ on the true labels of the data.\n",
    "\n",
    "2. Discuss your selection of the hyperparameter $\\Lambda$ and how that influences the confusion matrix $\\Theta$.\n",
    "\n",
    "3. Implement this Bayesian model and sample from the posterior to recover the underlying confusion matrix $\\Theta$ and the distribution of the true labels $\\rho$.  Note: $\\Theta$ is shared by all three workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here is your data\n",
    "\n",
    "reports = np.load(\"HW3/reports.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
