{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AM 207**: Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verena Kaynig-Fittkau and Pavlos Protopapas  <br>\n",
    "**Due: 11.59 P.M. Thursday April 14th, 2016**\n",
    "\n",
    "### Note: This homework is only for one week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "+ Upload your answers in an ipython notebook to Canvas.\n",
    "\n",
    "+ We will provide you imports for your ipython notebook. Please do not import additional libraries.\n",
    "\n",
    "+ Your individual submissions should use the following filenames: AM207_YOURNAME_HW5.ipynb\n",
    "\n",
    "+ Your code should be in code cells as part of your ipython notebook. Do not use a different language (or format). \n",
    "\n",
    "+ **Do not just send your code. The homework solutions should be in a report style. Be sure to add comments to your code as well as markdown cells where you describe your approach and discuss your results. **\n",
    "\n",
    "+ Please submit your notebook in an executed status, so that we can see all the results you computed. However, we will still run your code and all cells should reproduce the output when executed. \n",
    "\n",
    "+ If you have multiple files (e.g. you've added code files or images) create a tarball for all files in a single file and name it: AM207_YOURNAME_HW5.tar.gz or AM207_YOURNAME_HW5.zip\n",
    "\n",
    "\n",
    "### Have Fun!\n",
    "_ _ _ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import scipy.stats \n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: HMM... I Think Your Text Got Corrupted!\n",
    "\n",
    "In this problem you should use a Hidden Markov Model to correct typos in a text without using a dictionary. Your data is in two different text files:\n",
    "\n",
    "* `Shakespeare_correct.txt` contains the words of some sonnets from Shakespeare\n",
    "* `Shakespeare_typos.txt` contains the same text, but now some of the characters are corrupted\n",
    "\n",
    "For convenience both text files only contain lower case letters a-z and spaces. \n",
    "\n",
    "First build a first order HMM:\n",
    "* What are the hidden states and what are the observed states?\n",
    "* What should you do to generate your HMM probability matrices?\n",
    "* For some of the HMM parameters, you won't have enough training data to get representative probabilities.  For example, some of your probabilites might be 0. You should address this problem by adding a small pseudocount, similar to the motif finding problem from a previous assignment. \n",
    "* Implement the Viterbi algorithm and run it on a test portion that contains errors. Show that your Viterbi implementation can improve text of length 100, 500, 1000, and 2000. Note: To do this correctly you would have to withhold the part of the text that you use for testing when you estimate the parameters for you HMM. For the sake of this homework it is ok though to report training error instead of test error. Just be aware that the correction rate you are reporting most likely is a very optimistic estimate. \n",
    "* What correction rate do you get?\n",
    "\n",
    "**Important**: Wikipedia has a nice article on [Viterbi](https://en.wikipedia.org/wiki/Viterbi_algorithm). **Please do not use the python implementation from this article!** (The lecture notebook also has the version from Wikipedia). Using dictionaries for Viterbi is really not intuitive and using numpy is typically faster. The article has very nice pseudo code that should enable you to easily program Viterbi by yourself. Please also refrain for this problem from using any other third party implementations. \n",
    "\n",
    "Now for a second order HMM:\n",
    "By using a second order HMM, you should be able to get a better correction rate. \n",
    "* Give an intuitive explanation why a second order HMM should give better results.\n",
    "* Implement your second order text correction. Hint: If you think a bit about the model you won't even have to change your Viterbi implementation. \n",
    "* Compare your correction rates against the first order model for text length of 100 and 500, (you can do 1000 as well if your computer is fast enough). \n",
    "* How well would your implementation scale to HMMs of even higher order? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Problem 2: Final Project Review\n",
    "    \n",
    "You will be contacted shortly by a TF to meet and discuss your final project proposal. Be sure to take advantage of this feedback option. Review meetings should be scheduled within the week from April 11-15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of correct words =  11597\n",
      "number of typo words = 11597\n",
      "removing redundant words, number of distinct correct words is 2450\n",
      "removing redundant words, number of distinct typo words is 4940\n",
      "1st order HMM starts ...\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "correction rate = 0.98 \n",
      " corrected list = ['dye', 'as', 'the', 'grave', 'and', 'thee', 'when', 'forty', 'winters', 'shall', 'besiege', 'thy', 'brow', 'and', 'dig', 'deep', 'trenches', 'in', 'thy', 'beautys', 'field', 'thy', 'youths', 'proud', 'livery', 'so', 'gazed', 'on', 'now', 'will', 'be', 'a', 'totterd', 'weed', 'of', 'small', 'worth', 'held', 'then', 'being', 'asked', 'where', 'all', 'thy', 'beauty', 'lies', 'where', 'all', 'the', 'treasure', 'of', 'thy', 'lusty', 'days', 'to', 'say', 'within', 'thine', 'own', 'deep', 'sunken', 'eyes', 'were', 'an', 'alleating', 'shame', 'and', 'thriftless', 'praise', 'how', 'much', 'more', 'praise', 'deservd', 'thy', 'beautys', 'use', 'if', 'thou', 'couldst', 'answer', 'this', 'fair', 'child', 'of', 'mine', 'shall', 'sum', 'my', 'count', 'and', 'make', 'my', 'old', 'excuse', 'proving', 'his', 'beauty', 'by', 'succession'] \n",
      " original list= ['due', 'by', 'the', 'grave', 'and', 'thee', 'when', 'forty', 'winters', 'shall', 'besiege', 'thy', 'brow', 'and', 'dig', 'deep', 'trenches', 'in', 'thy', 'beautys', 'field', 'thy', 'youths', 'proud', 'livery', 'so', 'gazed', 'on', 'now', 'will', 'be', 'a', 'totterd', 'weed', 'of', 'small', 'worth', 'held', 'then', 'being', 'asked', 'where', 'all', 'thy', 'beauty', 'lies', 'where', 'all', 'the', 'treasure', 'of', 'thy', 'lusty', 'days', 'to', 'say', 'within', 'thine', 'own', 'deep', 'sunken', 'eyes', 'were', 'an', 'alleating', 'shame', 'and', 'thriftless', 'praise', 'how', 'much', 'more', 'praise', 'deservd', 'thy', 'beautys', 'use', 'if', 'thou', 'couldst', 'answer', 'this', 'fair', 'child', 'of', 'mine', 'shall', 'sum', 'my', 'count', 'and', 'make', 'my', 'old', 'excuse', 'proving', 'his', 'beauty', 'by', 'succession']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\na = [\"asd\",\"def\",\"ase\",\"dfg\",\"asd\",\"def\",\"dfg\"]\\na = list(set(a))\\nb = sorted(a)\\nprint a, \\'\\n\\', b\\n\\nimport operator\\nprint dist , dist2\\n\\nprint \\'emission prob\\', calc_emit_prob(\\'abc\\',[\\'abc\\',\\'asd\\'])\\nprint \\'emission prob\\', calc_emit_prob(\\'abc\\',[\\'abc\\',\\'asdc\\'])\\nprint \\'emission prob\\', calc_emit_prob(\\'abc\\',[\\'acb\\',\\'asd\\'])\\nprint \\'emission prob\\', calc_emit_prob(\\'abc\\',[\\'abe\\',\\'asd\\'])\\nprint b.index(\"def\")\\n\\nv= []\\nfor i in range(10):\\n    v.append(\\'a\\')\\nprint v\\n\\nfor (inda,suba) in enumerate(a):\\n    print inda, suba\\n\\n\\na_vec = [\\'asd\\',\\'sda\\',\\'sda\\']\\nb_vec = [\\'asd\\',\\'ccc\\',\\'sdc\\']\\n\\n\\nprint calc_correction_rate(a_vec,b_vec)\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in word sequences\n",
    "with open('Shakespeare_correct.txt', 'r') as myfile:\n",
    "    data=myfile.read()  \n",
    "correct_words = data.split(' ')\n",
    "\n",
    "with open('Shakespeare_typos.txt', 'r') as myfile:\n",
    "    data=myfile.read()  \n",
    "typo_words = data.split(' ')\n",
    "\n",
    "#testing\n",
    "#correct_words = ['a','from','forest','a','fzzn','gireot','qq']\n",
    "#typo_words = ['fron','firest','c','fzzp']\n",
    "\n",
    "print 'number of correct words = ',len(correct_words)\n",
    "print 'number of typo words =', len(typo_words)\n",
    "len_sequence = len(correct_words)\n",
    "\n",
    "#remove redundant elements from the list\n",
    "sorted_correct_words = sorted(list(set(correct_words)))\n",
    "num_correct_words = len(sorted_correct_words)\n",
    "print 'removing redundant words, number of distinct correct words is', num_correct_words\n",
    "\n",
    "#remove redundant elements from the list\n",
    "sorted_typo_words = sorted(list(set(typo_words)))\n",
    "num_typo_words = len(sorted_typo_words)\n",
    "print 'removing redundant words, number of distinct typo words is', num_typo_words\n",
    "\n",
    "#compute the 1st order transition matrix based on correct words\n",
    "T = np.zeros((num_correct_words,num_correct_words),dtype = 'float')\n",
    "for i in range(1,len_sequence):\n",
    "    row_zkm1 = sorted_correct_words.index(correct_words[i-1])\n",
    "    col_zk   = sorted_correct_words.index(correct_words[i])\n",
    "    T[row_zkm1,col_zk] += 1\n",
    "\n",
    "#normalize and add pseudocount\n",
    "for i in range(T.shape[0]):\n",
    "    temp_sum = np.sum(T[i,:])\n",
    "    if temp_sum > 0:\n",
    "        T[i,:] = T[i,:]/temp_sum\n",
    "    else:\n",
    "        T[i,:] = 1.0/T.shape[0]\n",
    "        #print 'here', 1/T.shape[0]\n",
    "#print T\n",
    "\n",
    "# generate P(x|z) on spot!!\n",
    "#compute the emission matrix \n",
    "#E = np.zeros(dtype='float')            \n",
    "\n",
    "#function that calculates emission probability\n",
    "#x: observed \n",
    "#z: hidden states\n",
    "def calc_emit_prob(x,states):\n",
    "    l = 0.01\n",
    "    emit_prob = np.zeros(len(states),dtype ='float')\n",
    "    for (indz,z) in enumerate(states):\n",
    "        if len(x) == len(z):\n",
    "            dist = len(x) - [s1 == s2 for (s1, s2) in zip(x, z)].count(True)\n",
    "            emit_prob[indz] = l ** dist / scipy.misc.factorial(l)\n",
    "        else:\n",
    "            emit_prob[indz] = 0\n",
    "    return emit_prob\n",
    "\n",
    "def calc_correction_rate(list_a,list_b):\n",
    "    num_words = len(list_a)\n",
    "    num_correct = 0;\n",
    "    for i in range(num_words):\n",
    "        if list_a[i] == list_b[i]:\n",
    "            num_correct += 1\n",
    "\n",
    "    return (num_correct + 0.0) / (num_words) +0.0\n",
    "    \n",
    "#numpy viterbi implementation:\n",
    "def viterbi(obs, states, start_p, trans_p):\n",
    "    \n",
    "    #initialize:\n",
    "    len_obs = len(obs)       #length of observation test string \n",
    "    num_states = len(states) #number of states, here 2450\n",
    "    V = np.zeros((len_obs,num_states),dtype = 'float') #log probabilities\n",
    "    path = np.zeros((len_obs,num_states),dtype = 'int') -1\n",
    "    \n",
    "    #initialize base case\n",
    "    '''\n",
    "    for (indz,z) in enumerate(states):\n",
    "        print indz, z\n",
    "        print np.log(start_p[indz])\n",
    "        print np.log(calc_emit_prob(obs[0],z))\n",
    "        V[0,indz] = np.log(start_p[indz]) + np.log(calc_emit_prob(obs[0],z))\n",
    "        path[0,indz] = indz\n",
    "    '''\n",
    "    V[0,:] = np.log(start_p) + np.log(calc_emit_prob(obs[0],states))\n",
    "    path[0,:] = np.arange(0,num_states)\n",
    "    \n",
    "    #print 'starting...', np.log(calc_emit_prob(obs[0],states))\n",
    "    \n",
    "    #print V[0.:]\n",
    "    #assert(0)\n",
    "    \n",
    "    #run viterbi for t>0 \n",
    "    for t in range(1,len(obs)):\n",
    "        #print t\n",
    "        log_emit_prob = np.log(calc_emit_prob(obs[t],states))\n",
    "        #print 't =' ,t ,', log_emit_prob=' ,log_emit_prob\n",
    "        \n",
    "        for (indz, z) in enumerate(states):\n",
    "\n",
    "            #log_prob_vec = V[t-1,:] + np.log(trans_p[:,indz].transpose()) + np.log(calc_emit_prob(obs[t],states))\n",
    "            log_prob_vec = V[t-1,:] + np.log(trans_p[:,indz].transpose()) + log_emit_prob[indz]\n",
    "            V[t,indz] = np.max(log_prob_vec)\n",
    "            path[t-1,indz] = np.argmax(log_prob_vec)\n",
    "            \n",
    "            #print 'z= ', z, ',transition prob =', trans_p[indz,:].transpose(), \\\n",
    "            #      ',emission prob = ', np.exp(log_emit_prob[indz]), ',path=', path, \\\n",
    "            #      ',V[t,indz] =', V[t,indz]\n",
    "    \n",
    "    #find best solution based on IC\n",
    "    #print 'writing solution...\\n'\n",
    "    best_ind = np.argmax(V[t,:])\n",
    "    path[t,best_ind] = best_ind\n",
    "    #print 'best indices is: ', best_ind\n",
    "    #print 'final path =', path\n",
    "    #best_log_prob = V[t,best_ind]\n",
    "    #best_path_ind = path[:,best_ind]\n",
    "    \n",
    "    best_path = [states[best_ind]]\n",
    "    col_ind = best_ind\n",
    "    for i in range(len(obs)-2,-1,-1):\n",
    "        col_ind = path[i,col_ind]\n",
    "        best_path.append(states[col_ind])\n",
    "    \n",
    "    best_path.reverse()\n",
    "    \n",
    "    #print best_path\n",
    "    #print 'V=', V\n",
    "    #print best_log_prob\n",
    "    #print 'best_path =', best_path\n",
    "    #print 'obs =', obs\n",
    "    return best_log_prob, best_path\n",
    "\n",
    "#driver script\n",
    "if __name__ == '__main__':\n",
    "    print '1st order HMM starts ...\\n'\n",
    "    start_p = np.ones(len(sorted_correct_words),dtype = 'float')\n",
    "    #print 'typo words are, ', typo_words[0:10]\n",
    "    #print 'states are', sorted_correct_words\n",
    "    best_log_prob, best_path = viterbi(typo_words[100:200],sorted_correct_words,start_p,T)\n",
    "    correct_rate = calc_correction_rate(best_path,correct_words[100:200])\n",
    "    print 'correction rate =', correct_rate, '\\n corrected list =', best_path, '\\n original list=', correct_words[100:200]\n",
    "\n",
    "\n",
    "\n",
    "#testing code\n",
    "'''\n",
    "a = [\"asd\",\"def\",\"ase\",\"dfg\",\"asd\",\"def\",\"dfg\"]\n",
    "a = list(set(a))\n",
    "b = sorted(a)\n",
    "print a, '\\n', b\n",
    "\n",
    "import operator\n",
    "print dist , dist2\n",
    "\n",
    "print 'emission prob', calc_emit_prob('abc',['abc','asd'])\n",
    "print 'emission prob', calc_emit_prob('abc',['abc','asdc'])\n",
    "print 'emission prob', calc_emit_prob('abc',['acb','asd'])\n",
    "print 'emission prob', calc_emit_prob('abc',['abe','asd'])\n",
    "print b.index(\"def\")\n",
    "\n",
    "v= []\n",
    "for i in range(10):\n",
    "    v.append('a')\n",
    "print v\n",
    "\n",
    "for (inda,suba) in enumerate(a):\n",
    "    print inda, suba\n",
    "\n",
    "\n",
    "a_vec = ['asd','sda','sda']\n",
    "b_vec = ['asd','ccc','sdc']\n",
    "\n",
    "\n",
    "print calc_correction_rate(a_vec,b_vec)\n",
    "'''\n",
    "#print num_correct\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
