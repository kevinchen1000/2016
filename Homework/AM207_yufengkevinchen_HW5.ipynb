{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AM 207**: Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verena Kaynig-Fittkau and Pavlos Protopapas  <br>\n",
    "**Due: 11.59 P.M. Thursday April 14th, 2016**\n",
    "\n",
    "### Note: This homework is only for one week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "+ Upload your answers in an ipython notebook to Canvas.\n",
    "\n",
    "+ We will provide you imports for your ipython notebook. Please do not import additional libraries.\n",
    "\n",
    "+ Your individual submissions should use the following filenames: AM207_YOURNAME_HW5.ipynb\n",
    "\n",
    "+ Your code should be in code cells as part of your ipython notebook. Do not use a different language (or format). \n",
    "\n",
    "+ **Do not just send your code. The homework solutions should be in a report style. Be sure to add comments to your code as well as markdown cells where you describe your approach and discuss your results. **\n",
    "\n",
    "+ Please submit your notebook in an executed status, so that we can see all the results you computed. However, we will still run your code and all cells should reproduce the output when executed. \n",
    "\n",
    "+ If you have multiple files (e.g. you've added code files or images) create a tarball for all files in a single file and name it: AM207_YOURNAME_HW5.tar.gz or AM207_YOURNAME_HW5.zip\n",
    "\n",
    "\n",
    "### Have Fun!\n",
    "_ _ _ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import scipy.stats \n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: HMM... I Think Your Text Got Corrupted!\n",
    "\n",
    "In this problem you should use a Hidden Markov Model to correct typos in a text without using a dictionary. Your data is in two different text files:\n",
    "\n",
    "* `Shakespeare_correct.txt` contains the words of some sonnets from Shakespeare\n",
    "* `Shakespeare_typos.txt` contains the same text, but now some of the characters are corrupted\n",
    "\n",
    "For convenience both text files only contain lower case letters a-z and spaces. \n",
    "\n",
    "First build a first order HMM:\n",
    "* What are the hidden states and what are the observed states?\n",
    "* What should you do to generate your HMM probability matrices?\n",
    "* For some of the HMM parameters, you won't have enough training data to get representative probabilities.  For example, some of your probabilites might be 0. You should address this problem by adding a small pseudocount, similar to the motif finding problem from a previous assignment. \n",
    "* Implement the Viterbi algorithm and run it on a test portion that contains errors. Show that your Viterbi implementation can improve text of length 100, 500, 1000, and 2000. Note: To do this correctly you would have to withhold the part of the text that you use for testing when you estimate the parameters for you HMM. For the sake of this homework it is ok though to report training error instead of test error. Just be aware that the correction rate you are reporting most likely is a very optimistic estimate. \n",
    "* What correction rate do you get?\n",
    "\n",
    "**Important**: Wikipedia has a nice article on [Viterbi](https://en.wikipedia.org/wiki/Viterbi_algorithm). **Please do not use the python implementation from this article!** (The lecture notebook also has the version from Wikipedia). Using dictionaries for Viterbi is really not intuitive and using numpy is typically faster. The article has very nice pseudo code that should enable you to easily program Viterbi by yourself. Please also refrain for this problem from using any other third party implementations. \n",
    "\n",
    "Now for a second order HMM:\n",
    "By using a second order HMM, you should be able to get a better correction rate. \n",
    "* Give an intuitive explanation why a second order HMM should give better results.\n",
    "* Implement your second order text correction. Hint: If you think a bit about the model you won't even have to change your Viterbi implementation. \n",
    "* Compare your correction rates against the first order model for text length of 100 and 500, (you can do 1000 as well if your computer is fast enough). \n",
    "* How well would your implementation scale to HMMs of even higher order? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "    \n",
    "We implement first order and second order HMM to correct the typos. \n",
    "\n",
    "### First order HMM\n",
    "Here the hidden states $z_k$ is the estimated (or corrected) letter at location $k$ of the sequence. The observed states $x_k$ is the observed letter at location $k$ of the typo sequence. \n",
    "\n",
    "We need to generate the transition matrix $T \\in R^{27\\times27}$ and the emission matrix $ E \\in R^{27\\times27}$. Here we work in the unit of letters. There are 60208 letters in the given texts. The matrix is 27 by 27 because there are 26 letters and 1 space. We can calculate $T$ such that $T_{ij} = P(j|i)$, where j and i are the letter is the alphabet or space. j corresponds to the letter at $z_k$, and i corresponds to the letter at $z_{k-1}$.  We simply loop through the sequence of correct letters, look at every consecutive pair of $z_k$ and $z_{k+1}$, and increment the corresponding $T_{ij}$ by 1. \n",
    "\n",
    "Similarly, we can generate the emission matrix $E$ by looking at the correct sequence and the typo sequence. Here the matrix is 27 by 27 because there are 26 letters and 1 space. We can calculate $E$ such that $E_{ij} = P(j|i)$, where j and i are the letter is the alphabet or space. j corresponds to the letter at $x_k$, and i corresponds to the letter at $z_k$. For each pair of $z_k$ and $x_k$, we increment of the corresponding element  $E_{ij}$ by 1. \n",
    "\n",
    "Following the instruction, we add a pseudocount of 0.1 to all elements of $E$ and $T$. Finally, we normalize the rows of E and T such that $\\sum_j E_{ij} = 1$ and $\\sum_j T_{ij} = 1$.  \n",
    "\n",
    "Next we implement of viterbi algorithm using numpy arrays. For implmentation details please refer to the attached code. The core recursive step is \n",
    "\n",
    "$$ V_t(z_k) =  \\max_{z_{k-1}} T_{kj} \\times V_{t-1}(z_{k}) \\times E_{x_k,z_k} $$, \n",
    "where $V_t(z_k)$ is the maximum probability of the hidden state is $z_k$ at time step t. To avoid numerical instability, we work in log scale instead. \n",
    "\n",
    "To avoid re-computing $T$ and $E$ for each test case, we report training error which is based on the same T and E matrix. We find the typo substring usually has correction rate of around 0.8. We use viterbi implementation to predict the hidden states and look at the correction rate of the corrected text. Here we define the correction rate as:\n",
    "\n",
    "$$ R_{correction} = \\frac{typo\\_error - corrected\\_error}{typo\\_error} = \\frac{corrected\\_correct\\_rate - typo\\_correct\\_rate}{1-typo\\_correct\\_rate}$$\n",
    "\n",
    "For fair comparison, we run the same algorithm on several segements of equal length with random starting position to estimate $R_{correction}$ mean and standard deviation for different length. The plots of 1st order HMM and 2nd order HMM (using same typo input string) results are attached with the code. The results of 1st order HMM are:\n",
    "\n",
    "$$ length = 100 , \\ \\ \\ mean(R)=0.63, \\ \\ \\ std(R)= 0.15$$\n",
    "$$ length = 500 , \\ \\ \\ mean(R)=0.63, \\ \\ \\ std(R)=0.062$$\n",
    "$$ length = 1000 , \\ \\ \\ mean(R)=0.63, \\ \\ \\ std(R)=0.048$$\n",
    "$$ length = 2000 , \\ \\ \\ mean(R)=0.63, \\ \\ \\ std(R)=0.036$$\n",
    "\n",
    "Overall, the correction rate of 1st order HMM is around $63\\%$.\n",
    "\n",
    "### Second order HMM\n",
    "\n",
    "Intuitively we expect second order HMM to yield better correction rate because we are using the previous 2 states to predict the current state. In English spelling there are roots that make prediction more accurate. Words of length 3 or shorter may also benefit. For instance, after seeing 'is', it's likely to be followed by a space ' '. After seeing 'ar', it maybe more likely to see the next letter being 'e'. However, we need to make sure that the training set is large enough since there are much more states. \n",
    "\n",
    "Please refer to the attached implmentation below. Here we use the same viterbi implmentation to do 1st order and 2nd order HMM. The function is identical. The only differences are the input states and probability matrices $T$ and $E$. Here the transition matrix $T$ and emission matrix $E$ are of size $27^2 \\times 27^2$. We formulate every possible pair and then update the appropriate entries by looping through the correct and typo sequences. Note that the second order HMM have more hidden states and hence is more computationally intensive. For details of how to generat $T$ and $E$, please refer to the attached code. Note that it is possible to reduce memory usage by using sparse matrices. We know most elements of the transition probability $T$ are zero if the second letter of ${z_{k-1},z_k}$is different from the first letter of ${z_{k},z_{k+1}}$. (i.e $T: P(ab|cd) = 0$)\n",
    "\n",
    "My implementation of viterbi will remain the same when scaling to even higher HMM. However, the computational cost will become infeasible since the number of states will grow as the power of the order. If we want to implement higher order models then we need to use sparse matrices and make simplifications, possibly ruling out infeasible states.  \n",
    "\n",
    "For fair comparison, we run the same algorithm on several segements of equal length with random starting position to estimate $R_{correction}$ mean and standard deviation for different length. The plots of 1st order HMM and 2nd order HMM (using same typo input string) results are attached with the code. The results of 2nd order HMM are:\n",
    "\n",
    "$$ length = 100 , \\ \\ \\ mean(R)=0.975, \\ \\ \\ std(R)=0.018$$\n",
    "$$ length = 500 , \\ \\ \\ mean(R)=0.975, \\ \\ \\ std(R)=0.0083$$\n",
    "$$ length = 1000 , \\ \\ \\ mean(R)=0.975, \\ \\ \\ std(R)=0.0052$$\n",
    "$$ length = 2000 , \\ \\ \\ mean(R)=0.975, \\ \\ \\ std(R)=0.0047$$\n",
    "\n",
    "Overall, the correction rate of 2nd order HMM is around $97\\%$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of correct letter is, 60208\n",
      "length of typo letter is 60208\n",
      "setting up comparison sequences ... \n",
      "1st order HMM starts ...\n",
      "\n",
      "[[ 0.7         0.53333333  0.72727273  0.57142857  0.61538462  0.77777778\n",
      "   0.64705882  0.73684211  0.61538462  0.22222222  0.25        0.6875\n",
      "   0.77777778  0.58823529  0.5         0.8125      0.35714286  0.63157895\n",
      "   0.63157895  0.4         0.83333333  0.54545455  0.4         0.76470588\n",
      "   0.6         0.29411765  0.73333333  0.52941176  0.33333333  0.66666667\n",
      "   0.5         0.5         0.5         1.          0.53333333  0.76923077\n",
      "   0.78571429  0.6         0.5         0.52380952  0.5625      0.08333333\n",
      "   0.375       0.30769231  0.66666667  0.75        0.65        0.75\n",
      "   0.72727273  0.73684211  0.375       0.42105263  0.73333333  0.73333333\n",
      "   0.75        0.73913043  0.52941176  0.88888889  0.5         0.75\n",
      "   0.68421053  0.61538462  0.23529412  0.72727273  0.61538462  0.41666667\n",
      "   0.55555556  0.61904762  0.61538462  0.77777778  0.63157895  0.81818182\n",
      "   0.53846154  0.82608696  0.61904762  0.61538462  0.76470588  0.94117647\n",
      "   0.8         0.52941176  0.80952381  0.75        0.625       0.5         0.5625\n",
      "   0.68421053  0.64285714  0.64285714  0.33333333  0.77777778  0.46153846\n",
      "   0.21052632  0.9375      0.72727273  0.6875      0.61111111  0.76923077\n",
      "   0.63157895  0.72727273  0.61111111  0.64705882  0.61111111  0.72727273\n",
      "   0.70588235  0.53333333  0.71428571  0.57894737  0.71428571  0.8\n",
      "   0.58333333  0.63636364  0.75        0.83333333  0.875       0.38461538\n",
      "   0.91666667  0.75        0.625       0.71428571  0.83333333  0.86666667\n",
      "   0.71428571  0.44444444  0.5         0.63157895  0.7         0.85714286\n",
      "   0.4         0.57894737  0.62962963  0.68181818  0.70588235  0.61538462\n",
      "   0.35294118  0.64285714  0.53333333  0.625       0.75        0.7\n",
      "   0.53846154  0.73333333  0.53846154  0.61538462  0.78571429  0.73333333\n",
      "   0.83333333  0.46666667  0.76470588  0.47058824  0.66666667  0.88888889\n",
      "   0.76470588  0.84210526  0.53846154  0.5         0.5         0.71428571\n",
      "   0.69230769  0.33333333  0.75        0.59090909  0.85714286  0.77777778\n",
      "   0.625       0.6         0.5625      0.58333333  0.70588235  0.72222222\n",
      "   0.72222222  0.72222222  0.58333333  0.54545455  0.63636364  0.4\n",
      "   0.53333333  0.76        0.5625      0.4375      0.33333333  0.625\n",
      "   0.70588235  0.71428571  0.66666667  0.57142857  0.78947368  0.6875\n",
      "   0.625       0.82352941  0.52941176  0.72727273  0.625       0.60869565\n",
      "   0.4         0.5         0.6         0.6         0.35714286  0.77777778\n",
      "   0.57142857]\n",
      " [ 0.62637363  0.63157895  0.52325581  0.61627907  0.62162162  0.53333333\n",
      "   0.73417722  0.69047619  0.6025641   0.60465116  0.75        0.6744186\n",
      "   0.67058824  0.5         0.62195122  0.62666667  0.50746269  0.64102564\n",
      "   0.675       0.64864865  0.73033708  0.64197531  0.59550562  0.69473684\n",
      "   0.68888889  0.64444444  0.6344086   0.62857143  0.68604651  0.60294118\n",
      "   0.64367816  0.69863014  0.72463768  0.6835443   0.70238095  0.65822785\n",
      "   0.67816092  0.5862069   0.6744186   0.63855422  0.62820513  0.60606061\n",
      "   0.48780488  0.68604651  0.57317073  0.5862069   0.63636364  0.66666667\n",
      "   0.6375      0.63953488  0.6375      0.62121212  0.67948718  0.64772727\n",
      "   0.75609756  0.5         0.65432099  0.57894737  0.64634146  0.54666667\n",
      "   0.66233766  0.61956522  0.55882353  0.56321839  0.58974359  0.69736842\n",
      "   0.67045455  0.56962025  0.7311828   0.68        0.61363636  0.50561798\n",
      "   0.64444444  0.73493976  0.63636364  0.63953488  0.63636364  0.6025641\n",
      "   0.58139535  0.56        0.70731707  0.59090909  0.68235294  0.50632911\n",
      "   0.63855422  0.52173913  0.65853659  0.72619048  0.6125      0.67741935\n",
      "   0.65432099  0.63157895  0.71052632  0.6741573   0.64383562  0.49382716\n",
      "   0.625       0.65306122  0.63414634  0.5862069   0.68539326  0.58888889\n",
      "   0.65882353  0.64788732  0.69473684  0.58974359  0.67142857  0.69767442\n",
      "   0.609375    0.64367816  0.61643836  0.64285714  0.60714286  0.73076923\n",
      "   0.56756757  0.68539326  0.55555556  0.7         0.78651685  0.56756757\n",
      "   0.60273973  0.65909091  0.58441558  0.7195122   0.62765957  0.48780488\n",
      "   0.72972973  0.66233766  0.63529412  0.53246753  0.55696203  0.67123288\n",
      "   0.65217391  0.68817204  0.57746479  0.67857143  0.70526316  0.66666667\n",
      "   0.6043956   0.60810811  0.71428571  0.63768116  0.6344086   0.65277778\n",
      "   0.55555556  0.75324675  0.6547619   0.61111111  0.68965517  0.63636364\n",
      "   0.4556962   0.69230769  0.66666667  0.50666667  0.54929577  0.68918919\n",
      "   0.59210526  0.69047619  0.71264368  0.44871795  0.57692308  0.55128205\n",
      "   0.63157895  0.6875      0.69148936  0.69230769  0.63636364  0.56666667\n",
      "   0.63953488  0.58536585  0.71590909  0.63235294  0.625       0.54411765\n",
      "   0.66233766  0.60294118  0.66666667  0.66666667  0.72619048  0.74683544\n",
      "   0.625       0.5974026   0.55        0.57894737  0.6741573   0.67816092\n",
      "   0.63157895  0.65217391  0.61363636  0.54545455  0.6626506   0.63888889\n",
      "   0.62121212  0.56321839  0.58227848  0.67948718  0.7         0.58024691\n",
      "   0.68831169  0.5875    ]\n",
      " [ 0.65605096  0.6625      0.67261905  0.62804878  0.64367816  0.63190184\n",
      "   0.62142857  0.59090909  0.58024691  0.56862745  0.67307692  0.67058824\n",
      "   0.64457831  0.65714286  0.73563218  0.71428571  0.58928571  0.66298343\n",
      "   0.66451613  0.64375     0.63580247  0.63253012  0.65294118  0.67241379\n",
      "   0.59119497  0.61046512  0.56862745  0.66666667  0.64912281  0.53374233\n",
      "   0.64285714  0.63333333  0.65317919  0.70625     0.6125      0.62025316\n",
      "   0.61818182  0.69461078  0.62937063  0.67836257  0.66878981  0.57303371\n",
      "   0.60795455  0.69426752  0.60606061  0.6626506   0.74172185  0.61842105\n",
      "   0.7027027   0.64942529  0.55625     0.67052023  0.64497041  0.64556962\n",
      "   0.56375839  0.74390244  0.68361582  0.59887006  0.57317073  0.60645161\n",
      "   0.73125     0.66081871  0.64071856  0.5988024   0.68674699  0.61038961\n",
      "   0.65584416  0.62420382  0.62732919  0.66666667  0.60493827  0.67045455\n",
      "   0.57407407  0.67549669  0.65189873  0.62111801  0.59302326  0.6025641\n",
      "   0.64827586  0.59375     0.575       0.64367816  0.48125     0.60958904\n",
      "   0.58928571  0.64556962  0.59748428  0.57831325  0.58169935  0.64088398\n",
      "   0.6516129   0.62711864  0.63218391  0.65497076  0.74390244  0.71604938\n",
      "   0.52380952  0.61325967  0.68        0.64571429  0.62721893  0.65189873\n",
      "   0.59119497  0.63583815  0.64596273  0.69655172  0.59589041  0.62424242\n",
      "   0.64197531  0.67741935  0.55555556  0.66451613  0.63333333  0.60119048\n",
      "   0.65605096  0.62251656  0.68987342  0.61832061  0.50887574  0.63541667\n",
      "   0.53424658  0.65384615  0.47826087  0.59440559  0.64084507  0.6374269\n",
      "   0.65269461  0.62721893  0.7483871   0.63905325  0.59281437  0.59748428\n",
      "   0.64457831  0.57763975  0.62857143  0.57419355  0.63522013  0.58333333\n",
      "   0.62121212  0.60946746  0.56804734  0.64912281  0.57333333  0.64596273\n",
      "   0.62745098  0.63253012  0.66860465  0.67515924  0.64        0.67582418\n",
      "   0.63803681  0.59863946  0.6         0.63565891  0.64417178  0.60493827\n",
      "   0.6402439   0.62758621  0.65644172  0.68508287  0.7125      0.65317919\n",
      "   0.60897436  0.67213115  0.56291391  0.65104167  0.625       0.64534884\n",
      "   0.60119048  0.61940299  0.66463415  0.54545455  0.69277108  0.61006289\n",
      "   0.65432099  0.57317073  0.7515528   0.63522013  0.59354839  0.64375\n",
      "   0.68387097  0.61142857  0.66101695  0.62937063  0.6196319   0.58536585\n",
      "   0.64971751  0.57534247  0.56329114  0.69090909  0.51875     0.56603774\n",
      "   0.65217391  0.55033557  0.64705882  0.65806452  0.63448276  0.66850829\n",
      "   0.65605096  0.58974359]\n",
      " [ 0.61682243  0.66086957  0.60197368  0.64396285  0.6547619   0.63897764\n",
      "   0.54458599  0.61180124  0.58860759  0.66349206  0.62269939  0.59393939\n",
      "   0.58160237  0.58745875  0.70063694  0.67692308  0.69871795  0.6637931\n",
      "   0.54140127  0.64285714  0.59365079  0.57142857  0.69426752  0.64485981\n",
      "   0.65838509  0.62101911  0.58769231  0.6443769   0.68923077  0.6115942\n",
      "   0.67584098  0.625       0.66049383  0.5955414   0.68789809  0.68\n",
      "   0.59756098  0.65123457  0.61398176  0.6223565   0.66769231  0.57878788\n",
      "   0.62745098  0.66990291  0.61676647  0.63692308  0.6035503   0.67647059\n",
      "   0.59516616  0.6377709   0.64457831  0.64011799  0.67045455  0.65269461\n",
      "   0.57928803  0.61919505  0.65361446  0.64848485  0.61538462  0.56495468\n",
      "   0.64637681  0.66180758  0.70382166  0.65846154  0.58695652  0.55660377\n",
      "   0.61094225  0.60416667  0.6637931   0.7047619   0.66776316  0.67246377\n",
      "   0.6102719   0.59283388  0.625       0.66193182  0.59210526  0.62393162\n",
      "   0.66358025  0.65749235  0.61861862  0.62130178  0.55140187  0.63636364\n",
      "   0.65165165  0.59442724  0.63608563  0.61688312  0.61656442  0.68852459\n",
      "   0.60960961  0.62385321  0.65361446  0.6         0.61111111  0.64724919\n",
      "   0.65757576  0.60244648  0.66885246  0.63467492  0.65963855  0.61585366\n",
      "   0.55140187  0.61038961  0.63444109  0.65994236  0.65811966  0.68627451\n",
      "   0.58333333  0.70700637  0.60298507  0.62345679  0.6130031   0.62006079\n",
      "   0.59501558  0.61609907  0.61682243  0.66571429  0.62089552  0.63897764\n",
      "   0.6977492   0.61585366  0.63501484  0.65443425  0.68923077  0.61515152\n",
      "   0.61651917  0.61607143  0.671875    0.60060976  0.59876543  0.63430421\n",
      "   0.63829787  0.65137615  0.6741573   0.60990712  0.62269939  0.64615385\n",
      "   0.56707317  0.59815951  0.625387    0.64525994  0.60655738  0.64831804\n",
      "   0.6         0.66086957  0.59223301  0.62691131  0.66076696  0.6379822\n",
      "   0.63253012  0.66163142  0.52777778  0.596875    0.6686747   0.69620253\n",
      "   0.62762763  0.6625      0.65921788  0.68        0.6006192   0.65088757\n",
      "   0.65853659  0.64423077  0.60802469  0.63714286  0.64217252  0.66564417\n",
      "   0.65256798  0.59146341  0.66376812  0.60895522  0.6402439   0.66666667\n",
      "   0.68373494  0.63037249  0.63076923  0.65363128  0.59872611  0.63862928\n",
      "   0.6         0.69902913  0.60237389  0.6627566   0.65578635  0.65203762\n",
      "   0.61890244  0.69158879  0.63354037  0.57098765  0.60856269  0.61846154\n",
      "   0.7047619   0.68141593  0.59090909  0.56325301  0.60990712  0.56402439\n",
      "   0.67346939  0.57621951]]\n",
      "\n",
      "2nd order HMM starts ...\n",
      "[[ 0.99    0.98    0.97    0.98    0.97    0.97    0.96    0.99    0.96\n",
      "   0.98    0.97    0.98    0.97    0.95    0.94    0.99    0.98    0.97\n",
      "   0.98    0.98    1.      0.88    0.96    0.98    0.97    0.95    0.98\n",
      "   0.98    0.96    0.97    0.99    0.94    0.97    1.      0.96    0.98    1.\n",
      "   0.99    0.99    0.95    0.98    0.96    0.98    0.98    0.99    0.99\n",
      "   0.96    0.98    0.98    0.99    0.96    0.95    0.97    0.98    0.99\n",
      "   0.98    0.97    0.98    0.99    0.98    0.97    0.98    0.92    0.96\n",
      "   0.95    0.96    0.97    0.99    0.98    0.98    0.96    0.99    0.98\n",
      "   0.96    0.97    0.97    0.97    1.      1.      0.97    0.97    0.99\n",
      "   0.97    0.96    0.99    0.97    0.98    0.96    0.94    0.96    0.97\n",
      "   0.91    0.99    0.98    1.      0.96    0.99    0.98    1.      0.97\n",
      "   0.97    0.94    0.97    0.95    0.95    0.98    0.97    0.99    0.99\n",
      "   0.99    0.95    0.96    0.99    0.97    0.95    0.99    0.97    0.96\n",
      "   0.98    1.      1.      0.97    1.      0.93    0.98    1.      1.      0.96\n",
      "   0.97    0.96    0.98    0.97    0.99    0.97    0.97    0.97    0.98\n",
      "   0.97    0.97    0.99    0.98    0.96    0.96    0.99    0.97    0.99\n",
      "   0.95    1.      0.97    0.96    0.96    0.95    0.99    0.98    1.      0.98\n",
      "   0.96    0.93    0.96    1.      0.97    1.      0.97    0.99    0.97\n",
      "   0.99    1.      0.97    0.98    0.99    1.      0.98    0.96    0.99\n",
      "   0.99    0.98    0.98    0.99    0.96    0.96    0.99    0.96    0.97    1.\n",
      "   0.98    0.98    0.99    0.99    1.      1.      0.98    1.      0.98\n",
      "   0.93    0.99    0.99    0.99    0.96    0.99    0.98  ]\n",
      " [ 0.976   0.97    0.968   0.974   0.968   0.956   0.968   0.99    0.974\n",
      "   0.97    0.986   0.978   0.972   0.964   0.968   0.972   0.976   0.97\n",
      "   0.982   0.976   0.99    0.978   0.968   0.978   0.97    0.974   0.964\n",
      "   0.978   0.972   0.98    0.986   0.992   0.992   0.978   0.982   0.978\n",
      "   0.976   0.97    0.98    0.974   0.972   0.982   0.97    0.97    0.966\n",
      "   0.974   0.98    0.984   0.976   0.972   0.976   0.982   0.984   0.982\n",
      "   0.986   0.96    0.972   0.986   0.984   0.978   0.974   0.966   0.96\n",
      "   0.972   0.972   0.974   0.968   0.974   0.97    0.978   0.972   0.956\n",
      "   0.962   0.984   0.974   0.986   0.988   0.974   0.976   0.986   0.962\n",
      "   0.984   0.974   0.958   0.984   0.964   0.984   0.982   0.976   0.964\n",
      "   0.978   0.972   0.984   0.974   0.968   0.968   0.974   0.97    0.962\n",
      "   0.964   0.966   0.966   0.978   0.98    0.964   0.974   0.988   0.97\n",
      "   0.978   0.982   0.982   0.984   0.974   0.976   0.98    0.98    0.964\n",
      "   0.974   0.986   0.968   0.976   0.978   0.964   0.986   0.968   0.956\n",
      "   0.984   0.978   0.98    0.984   0.976   0.986   0.964   0.98    0.978\n",
      "   0.968   0.976   0.978   0.982   0.984   0.984   0.982   0.964   0.988\n",
      "   0.974   0.98    0.978   0.984   0.98    0.978   0.948   0.97    0.976\n",
      "   0.974   0.984   0.978   0.97    0.98    0.98    0.94    0.976   0.97\n",
      "   0.964   0.97    0.98    0.988   0.978   0.97    0.968   0.956   0.978\n",
      "   0.982   0.98    0.97    0.986   0.98    0.978   0.984   0.974   0.992\n",
      "   0.982   0.966   0.974   0.976   0.968   0.97    0.978   0.972   0.974\n",
      "   0.966   0.986   0.978   0.974   0.972   0.972   0.98    0.976   0.96\n",
      "   0.976   0.978 ]\n",
      " [ 0.983   0.978   0.972   0.975   0.969   0.979   0.977   0.978   0.973\n",
      "   0.976   0.973   0.975   0.974   0.972   0.982   0.977   0.973   0.975\n",
      "   0.975   0.976   0.973   0.967   0.974   0.972   0.976   0.974   0.972\n",
      "   0.972   0.981   0.97    0.983   0.965   0.981   0.984   0.971   0.983\n",
      "   0.982   0.975   0.979   0.975   0.972   0.97    0.966   0.98    0.98\n",
      "   0.975   0.987   0.979   0.967   0.981   0.973   0.976   0.98    0.971\n",
      "   0.975   0.98    0.979   0.973   0.972   0.977   0.979   0.97    0.969\n",
      "   0.975   0.981   0.973   0.981   0.975   0.969   0.971   0.973   0.976\n",
      "   0.971   0.987   0.98    0.974   0.962   0.973   0.979   0.985   0.975\n",
      "   0.972   0.958   0.976   0.974   0.979   0.975   0.969   0.974   0.967\n",
      "   0.973   0.974   0.964   0.968   0.98    0.975   0.976   0.976   0.974\n",
      "   0.969   0.981   0.967   0.972   0.975   0.976   0.987   0.975   0.975\n",
      "   0.979   0.978   0.971   0.973   0.97    0.979   0.98    0.974   0.983\n",
      "   0.984   0.967   0.974   0.975   0.969   0.957   0.971   0.979   0.985\n",
      "   0.976   0.975   0.988   0.977   0.977   0.973   0.983   0.971   0.976\n",
      "   0.974   0.973   0.976   0.974   0.978   0.969   0.972   0.975   0.976\n",
      "   0.98    0.969   0.972   0.98    0.972   0.974   0.977   0.973   0.975\n",
      "   0.98    0.975   0.973   0.984   0.984   0.978   0.965   0.988   0.973\n",
      "   0.968   0.977   0.971   0.975   0.979   0.978   0.983   0.974   0.979\n",
      "   0.968   0.98    0.975   0.978   0.969   0.99    0.98    0.973   0.979\n",
      "   0.977   0.972   0.973   0.977   0.972   0.971   0.971   0.975   0.975\n",
      "   0.975   0.968   0.976   0.982   0.975   0.98    0.97    0.975   0.976\n",
      "   0.983   0.978 ]\n",
      " [ 0.9645  0.975   0.9745  0.9775  0.98    0.9815  0.97    0.9745  0.974\n",
      "   0.9715  0.973   0.975   0.9725  0.9795  0.9845  0.9805  0.9845  0.981\n",
      "   0.9705  0.981   0.972   0.974   0.982   0.98    0.972   0.9755  0.974\n",
      "   0.9715  0.9835  0.973   0.981   0.9675  0.979   0.9715  0.9805  0.981\n",
      "   0.97    0.9805  0.9735  0.9725  0.979   0.973   0.9785  0.9825  0.969\n",
      "   0.969   0.9705  0.984   0.9745  0.978   0.9705  0.97    0.973   0.9705\n",
      "   0.9785  0.9725  0.977   0.9735  0.9645  0.973   0.9715  0.9735  0.983\n",
      "   0.978   0.9745  0.9705  0.975   0.9725  0.9715  0.983   0.9835  0.979\n",
      "   0.975   0.975   0.974   0.974   0.967   0.969   0.9795  0.9705  0.9745\n",
      "   0.977   0.965   0.9795  0.9705  0.973   0.977   0.9805  0.9755  0.9835\n",
      "   0.9705  0.9795  0.9765  0.9745  0.975   0.9805  0.9775  0.9745  0.983\n",
      "   0.9705  0.9765  0.9745  0.966   0.9685  0.973   0.976   0.977   0.9835\n",
      "   0.9745  0.985   0.9725  0.974   0.97    0.97    0.9735  0.97    0.9715\n",
      "   0.9725  0.973   0.979   0.984   0.9745  0.9695  0.967   0.983   0.9705\n",
      "   0.972   0.9725  0.9815  0.971   0.971   0.969   0.9765  0.977   0.972\n",
      "   0.9745  0.9735  0.9785  0.9705  0.9685  0.9715  0.9775  0.972   0.977\n",
      "   0.965   0.972   0.973   0.975   0.972   0.977   0.967   0.971   0.9635\n",
      "   0.975   0.9705  0.9815  0.975   0.9715  0.977   0.98    0.9715  0.975\n",
      "   0.976   0.97    0.9735  0.9765  0.9815  0.9685  0.9755  0.975   0.97\n",
      "   0.974   0.975   0.9725  0.9805  0.968   0.974   0.9765  0.9725  0.967\n",
      "   0.9765  0.9835  0.972   0.9725  0.9755  0.9775  0.978   0.984   0.98\n",
      "   0.9725  0.973   0.9705  0.983   0.9755  0.9745  0.971   0.966   0.971\n",
      "   0.9725  0.9725]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\na = [\"asd\",\"def\",\"ase\",\"dfg\",\"asd\",\"def\",\"dfg\"]\\na = list(set(a))\\nb = sorted(a)\\nprint a, \\'\\n\\', b\\n\\nimport operator\\nprint dist , dist2\\n\\nprint \\'emission prob\\', calc_emit_prob(\\'abc\\',[\\'abc\\',\\'asd\\'])\\nprint \\'emission prob\\', calc_emit_prob(\\'abc\\',[\\'abc\\',\\'asdc\\'])\\nprint \\'emission prob\\', calc_emit_prob(\\'abc\\',[\\'acb\\',\\'asd\\'])\\nprint \\'emission prob\\', calc_emit_prob(\\'abc\\',[\\'abe\\',\\'asd\\'])\\nprint b.index(\"def\")\\n\\nv= []\\nfor i in range(10):\\n    v.append(\\'a\\')\\nprint v\\n\\nfor (inda,suba) in enumerate(a):\\n    print inda, suba\\n\\n\\na_vec = [\\'asd\\',\\'sda\\',\\'sda\\']\\nb_vec = [\\'asd\\',\\'ccc\\',\\'sdc\\']\\n\\n\\nprint calc_correction_rate(a_vec,b_vec)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in word sequences\n",
    "with open('Shakespeare_correct.txt', 'r') as myfile:\n",
    "    data = myfile.read() \n",
    "correct_l_len = len(data)\n",
    "print 'length of correct letter is,', correct_l_len\n",
    "correct_letters = data\n",
    "\n",
    "\n",
    "with open('Shakespeare_typos.txt', 'r') as myfile:\n",
    "    data=myfile.read()  \n",
    "typo_len = len(data)\n",
    "typo_letters = data\n",
    "print 'length of typo letter is', typo_len\n",
    "\n",
    "\n",
    "#function to calculate transition matrix of letters and spaces\n",
    "def calc_1st_trans_prob(s1 = correct_letters):\n",
    "    #initialize \n",
    "    ref_string = ' abcdefghijklmnopqrstuvwxyz'\n",
    "    T = np.zeros((27,27),dtype = 'float')\n",
    "    for i in range(1,len(s1)):\n",
    "        row_zkm1 = ref_string.index(s1[i-1])\n",
    "        col_zk = ref_string.index(s1[i])\n",
    "        T[row_zkm1,col_zk] +=1\n",
    "        \n",
    "    #normalize and add pseudocount\n",
    "    pseudocount = 0.1\n",
    "    T += pseudocount\n",
    "    \n",
    "    for i in range(T.shape[0]):\n",
    "        T[i,:] = T[i,:] / np.sum(T[i,:])\n",
    "    \n",
    "    return T\n",
    "\n",
    "#calculate 2nd order transition matrix of letters and spaces\n",
    "def calc_2nd_trans_prob(s1 = correct_letters):\n",
    "    #initialize \n",
    "    ref_string = ' abcdefghijklmnopqrstuvwxyz'\n",
    "    T = np.zeros((27**2,27**2),dtype = 'float')\n",
    "    for i in range(2,len(s1)):\n",
    "        row_zkm1_zkm2 = ref_string.index(s1[i-2]) * 27 + ref_string.index(s1[i-1])\n",
    "        col_zk_zkm1 = ref_string.index(s1[i-1])*27 + ref_string.index(s1[i])\n",
    "        T[row_zkm1_zkm2,col_zk_zkm1] +=1\n",
    "        \n",
    "    #normalize and add pseudocount\n",
    "    pseudocount = 0.1\n",
    "    T += pseudocount\n",
    "    \n",
    "    for i in range(T.shape[0]):\n",
    "        T[i,:] = T[i,:] / np.sum(T[i,:])\n",
    "    \n",
    "    return T\n",
    "\n",
    "def generate_2nd_HMM_states():\n",
    "    states = ' abcdefghijklmnopqrstuvwxyz'\n",
    "    hidden_states = [None] * (27**2)\n",
    "    for i in range(len(states)):\n",
    "        for j in range(len(states)):\n",
    "            hidden_states[i*27 + j] = states[i] + states[j]\n",
    "    \n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "#function to generate second order observation inputs\n",
    "def generate_2nd_HMM_obs(obs):\n",
    "    obs_2nd_order = [None] * (len(obs)-1)\n",
    "    for i in range(len(obs)-1):\n",
    "        obs_2nd_order[i] = obs[i] + obs[i+1]\n",
    "\n",
    "    return obs_2nd_order\n",
    "\n",
    "#function to calculate emission probability matrix of letters and spaces\n",
    "def calc_emission_prob(s1 = correct_letters, s2 = typo_letters):\n",
    "    #initialize \n",
    "    ref_string = ' abcdefghijklmnopqrstuvwxyz'\n",
    "    E = np.zeros((27,27),dtype = 'float')\n",
    "    for i in range(0,len(s1)):\n",
    "        row_zk = ref_string.index(s1[i])\n",
    "        col_xk = ref_string.index(s2[i])\n",
    "        E[row_zk,col_xk] +=1\n",
    "        \n",
    "    #normalize and add pseudocount\n",
    "    pseudocount = 0.1\n",
    "    E += pseudocount\n",
    "    \n",
    "    for i in range(E.shape[0]):\n",
    "        E[i,:] = E[i,:] / np.sum(E[i,:])\n",
    "    \n",
    "    return E\n",
    "\n",
    "#function to calculate emission probability matrix of letters and spaces\n",
    "def calc_2nd_emission_prob(s1 = correct_letters, s2 = typo_letters):\n",
    "    #initialize \n",
    "    ref_string = ' abcdefghijklmnopqrstuvwxyz'\n",
    "    E = np.zeros((27**2,27**2),dtype = 'float')\n",
    "    for i in range(1,len(s1)):\n",
    "        row_zkzkm1 = ref_string.index(s1[i-1])*27 + ref_string.index(s1[i])\n",
    "        col_xkxkm1 = ref_string.index(s2[i-1])*27 + ref_string.index(s2[i])\n",
    "        E[row_zkzkm1,col_xkxkm1] +=1\n",
    "        \n",
    "    #normalize and add pseudocount\n",
    "    pseudocount = 0.1\n",
    "    E += pseudocount\n",
    "    \n",
    "    for i in range(E.shape[0]):\n",
    "        E[i,:] = E[i,:] / np.sum(E[i,:])\n",
    "    \n",
    "    return E\n",
    "\n",
    "#calculate correction rate\n",
    "def calc_correction_rate(ref_list,typo_list,corrected_list):\n",
    "    \n",
    "    typo_match_rate = calc_match_rate(ref_list,typo_list)\n",
    "    corr_match_rate = calc_match_rate(ref_list,corrected_list)\n",
    "    \n",
    "    return (corr_match_rate - typo_match_rate) / (1 - typo_match_rate)\n",
    "\n",
    "#calculate match rate\n",
    "def calc_match_rate(list_a,list_b):\n",
    "    num_words = len(list_a)\n",
    "    num_correct = 0;\n",
    "    for i in range(num_words):\n",
    "        if list_a[i] == list_b[i]:\n",
    "            num_correct += 1\n",
    "\n",
    "    return (num_correct + 0.0) / (num_words) +0.0\n",
    "    \n",
    "#numpy viterbi implementation:\n",
    "#note: hidden states and emission states should be the same!!\n",
    "def viterbi(obs, hidden_states,emission_states, start_p, trans_p, emit_p):\n",
    "    #initialize:\n",
    "    len_obs = len(obs)                  #length of observation test string \n",
    "    num_states = len(hidden_states)     #number of states, here 27 or 27**2\n",
    "    V = np.zeros((len_obs,num_states),dtype = 'float') #log probabilities\n",
    "    path = np.zeros((len_obs,num_states),dtype = 'int') -1\n",
    "    \n",
    "    #initialize base case\n",
    "    V[0,:] = np.log(start_p) + np.log(emit_p[:,emission_states.index(obs[0])].transpose())\n",
    "    \n",
    "    #run viterbi for t>0 \n",
    "    for t in range(1,len(obs)):\n",
    "        log_emit_prob = np.log(emit_p[:,emission_states.index(obs[t])].transpose())\n",
    "        \n",
    "        for (indz, z) in enumerate(emission_states):\n",
    "            log_prob_vec = V[t-1,:] + np.log(trans_p[:,indz].transpose()) + log_emit_prob[indz]\n",
    "            V[t,indz] = np.max(log_prob_vec)\n",
    "            path[t-1,indz] = np.argmax(log_prob_vec)     \n",
    "    \n",
    "    #find best solution based on IC\n",
    "    best_ind = np.argmax(V[t,:])\n",
    "    path[t,best_ind] = best_ind\n",
    "    best_log_prob = V[t,best_ind]\n",
    "    best_path = emission_states[best_ind]\n",
    "    \n",
    "    #back track to find best path\n",
    "    col_ind = best_ind\n",
    "    for i in range(len(obs)-2,-1,-1):\n",
    "        col_ind = path[i,col_ind]\n",
    "        best_path += emission_states[col_ind]\n",
    "    \n",
    "    #reverse best_path\n",
    "    best_path = best_path[::-1]\n",
    "\n",
    "    #return solution\n",
    "    return best_log_prob, best_path\n",
    "\n",
    "#driver script\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    print 'setting up comparison sequences ... '\n",
    "    #define correction length vector\n",
    "    correct_len_vec = [100,500,1000,2000]\n",
    "    repeat = 200\n",
    "    \n",
    "    \n",
    "    start_ind_mat = np.zeros((len(correct_len_vec),repeat),dtype = 'int')\n",
    "    #initialize segment starting position\n",
    "    for i in range(len(correct_len_vec)):\n",
    "        for j in range(repeat):\n",
    "            start_ind_mat[i,j] = np.random.randint(1,correct_l_len-correct_len_vec[i])\n",
    "    \n",
    "    ######### 1st order HMM ######################################################################\n",
    "    print '1st order HMM starts ...\\n'\n",
    "    \n",
    "    #generate 1st order transition and emission matrices\n",
    "    T_1st = calc_1st_trans_prob()\n",
    "    E_1st = calc_emission_prob()\n",
    "    \n",
    "    hidden_1st_states = list(' abcdefghijklmnopqrstuvwxyz')\n",
    "    emission_states = ' abcdefghijklmnopqrstuvwxyz'\n",
    "    start_p = np.ones(len(emission_states),dtype = 'float')\n",
    "    correct_rate_mat_1st = np.zeros(start_ind_mat.shape,dtype ='float')\n",
    "    \n",
    "    for i in range(start_ind_mat.shape[0]):\n",
    "        for j in range(start_ind_mat.shape[1]):\n",
    "            typo_string = typo_letters[start_ind_mat[i,j]:(start_ind_mat[i,j]+correct_len_vec[i])]\n",
    "            correct_string = correct_letters[start_ind_mat[i,j]:(start_ind_mat[i,j]+correct_len_vec[i])]\n",
    "            best_log_prob, best_path = viterbi(typo_string,hidden_1st_states,emission_states,start_p,T_1st,E_1st)\n",
    "            correct_rate_mat_1st[i,j] = calc_correction_rate(correct_string,typo_string,best_path)\n",
    "    #print correct_rate_mat_1st\n",
    "    \n",
    "    correct_1st_mean_rate_vec = np.mean(correct_rate_mat_1st,axis =1)\n",
    "    correct_1st_std_rate_vec = np.std(correct_rate_mat_1st,axis =1)\n",
    "       \n",
    "    #2nd order HMM\n",
    "    #print generate_2nd_HMM_states()\n",
    "    \n",
    "    print '\\n2nd order HMM starts ...'\n",
    "    correct_rate_mat_2nd = np.zeros(start_ind_mat.shape,dtype ='float')\n",
    "    hidden_2nd_states = generate_2nd_HMM_states()\n",
    "    emission_states = generate_2nd_HMM_states()\n",
    "    correct_rate_mat_2nd = np.zeros(start_ind_mat.shape,dtype ='float')\n",
    "    start_p = np.ones(len(emission_states),dtype = 'float')\n",
    "    \n",
    "    #generate 2nd order transition and emission matrices\n",
    "    T_2nd = calc_2nd_trans_prob()\n",
    "    E_2nd = calc_2nd_emission_prob()\n",
    "    \n",
    "    for i in range(start_ind_mat.shape[0]):\n",
    "        for j in range(start_ind_mat.shape[1]):\n",
    "            typo_string = generate_2nd_HMM_obs(typo_letters[(start_ind_mat[i,j]-1):((start_ind_mat[i,j])+correct_len_vec[i])])\n",
    "            correct_string = correct_letters[start_ind_mat[i,j]:(start_ind_mat[i,j]+correct_len_vec[i])]\n",
    "            best_log_prob, best_path = viterbi(typo_string,hidden_2nd_states,emission_states,start_p,T_2nd,E_2nd)\n",
    "            best_path = best_path[0::2]\n",
    "            correct_rate_mat_2nd[i,j] = calc_correction_rate(correct_string,typo_string,best_path)\n",
    "            \n",
    "    #print correct_rate_mat_2nd\n",
    "    \n",
    "    correct_2nd_mean_rate_vec = np.mean(correct_rate_mat_2nd,axis =1)\n",
    "    correct_2nd_std_rate_vec = np.std(correct_rate_mat_2nd,axis =1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.62837461  0.63358681  0.63127499  0.63231177]\n",
      "[ 0.15462156  0.06205764  0.04709343  0.03641045]\n",
      "[ 0.97465    0.975      0.975265   0.9746675]\n",
      "[ 0.01816253  0.00828493  0.00524068  0.00469741]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7oAAAGLCAYAAADpve5MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6190fbb810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display results\n",
    "fig = plt.figure(figsize = (16,6))\n",
    "draw_w = 250\n",
    "\n",
    "#1st oder HMM\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('1st order HMM')\n",
    "plt.xlabel('string length')\n",
    "plt.ylabel('correct rate')\n",
    "plt.bar(np.array(correct_len_vec)-draw_w/2,correct_1st_mean_rate_vec,width = draw_w)\n",
    "plt.errorbar(correct_len_vec,correct_1st_mean_rate_vec,yerr = correct_1st_std_rate_vec,fmt = 'o',color ='green')\n",
    "\n",
    "print correct_1st_mean_rate_vec\n",
    "print correct_1st_std_rate_vec\n",
    "\n",
    "#2nd order HMM\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('2nd order HMM')\n",
    "plt.xlabel('string length')\n",
    "plt.ylabel('correct rate')\n",
    "plt.bar(np.array(correct_len_vec)-draw_w/2,correct_2nd_mean_rate_vec,width = draw_w)\n",
    "plt.errorbar(correct_len_vec,correct_2nd_mean_rate_vec,yerr = correct_2nd_std_rate_vec,fmt = 'o',color ='green')\n",
    "\n",
    "print correct_2nd_mean_rate_vec\n",
    "print correct_2nd_std_rate_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Problem 2: Final Project Review\n",
    "    \n",
    "You will be contacted shortly by a TF to meet and discuss your final project proposal. Be sure to take advantage of this feedback option. Review meetings should be scheduled within the week from April 11-15. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
